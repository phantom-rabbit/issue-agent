import os
import argparse
from github import Github
from tqdm import tqdm
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langchain_core.embeddings import Embeddings


class LocalEmbeddings(Embeddings):
    def __init__(self, model_name: str = "BAAI/bge-small-zh"):
        self.model = SentenceTransformer(model_name)

    def embed_query(self, text: str):
        return self.model.encode(text)

    def embed_documents(self, texts: list[str]):
        return self.model.encode(texts)


def fetch_github_issues(repo_name: str, max_pages: int = 3, token: str = None, per_page: int = 10):
    """
    ä»æŒ‡å®šä»“åº“è·å– issue åˆ—è¡¨ï¼ˆå«æ ‡é¢˜ã€æ­£æ–‡ã€è¯„è®ºï¼‰
    ä½¿ç”¨å•ä¸€è¿›åº¦æ¡æ˜¾ç¤ºè¿›åº¦ï¼Œåˆ†é¡µé€»è¾‘ä¸ per_page å‚æ•°ä¸€è‡´ã€‚
    """
    g = Github(token or os.getenv("GITHUB_TOKEN"))
    repo = g.get_repo(repo_name)
    all_issues = []

    tqdm.write(f"ğŸ” æ­£åœ¨æ‹‰å–ä»“åº“ {repo_name} çš„ Issuesï¼ˆæœ€å¤š {max_pages} é¡µï¼Œæ¯é¡µ {per_page} æ¡ï¼‰...")

    max_count = max_pages * per_page
    issues = repo.get_issues(state="all", direction="desc", sort="created")
    pbar = tqdm(total=max_count, desc="ğŸ“¥ æ‹‰å– Issues", unit="æ¡", dynamic_ncols=True)

    try:
        for page in range(1, max_pages + 1):
            issues_page = issues.get_page(page - 1)
            if not issues_page:
                break

            tqdm.write(f"ğŸ“„ ç¬¬ {page} é¡µï¼Œå…± {len(issues_page)} æ¡")

            for issue in issues_page:
                if getattr(issue, "pull_request", None):
                    continue

                comments = [c.body for c in issue.get_comments()]
                text = f"# {issue.title}\n\n{issue.body or ''}\n\n" + "\n".join(comments)

                all_issues.append({
                    "number": issue.number,
                    "title": issue.title,
                    "body": issue.body,
                    "comments": comments,
                    "content": text,
                })

                pbar.update(1)
                if len(all_issues) >= max_count:
                    break

            if len(all_issues) >= max_count:
                break

    finally:
        pbar.close()

    tqdm.write(f"ğŸ“¦ å·²æ‹‰å– {len(all_issues)} æ¡ Issueï¼ˆå…± {page} é¡µï¼‰")
    return all_issues


# ===== æ„å»º Chroma å‘é‡æ•°æ®åº“ =====
def build_vector_db(repo_name: str, persist_dir: str = "data/chroma", max_pages: int = 3, token: str = None):
    """
    æ‹‰å–å†å² Issue å¹¶æ„å»º Chroma å‘é‡æ•°æ®åº“
    """
    issues = fetch_github_issues(repo_name, max_pages=max_pages, token=token)
    print(f"ğŸ“¦ æ‹‰å–å®Œæˆï¼Œå…± {len(issues)} æ¡ Issue")

    # === æ„å»º LangChain æ–‡æ¡£å¯¹è±¡ ===
    docs = [Document(page_content=i["content"], metadata={"title": i["title"], "id": i["number"]}) for i in issues]

    # === æ–‡æœ¬åˆ‡åˆ† ===
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    split_docs = splitter.split_documents(docs)
    print(f"âœ‚ï¸ å·²åˆ†å‰²ä¸º {len(split_docs)} æ¡æ–‡æœ¬å—ï¼Œå¼€å§‹ç”Ÿæˆå‘é‡...")

    # === å‘é‡æ¨¡å‹ ===
    embeddings = LocalEmbeddings("BAAI/bge-small-zh")

    # === ç”Ÿæˆå‘é‡ ===
    texts = [d.page_content for d in split_docs]
    metadatas = [d.metadata for d in split_docs]

    vectors = []
    for text in tqdm(texts, desc="ğŸ§  æ­£åœ¨ç”Ÿæˆå‘é‡", unit="æ®µ"):
        vector = embeddings.embed_query(text)
        vectors.append(vector)

    print("ğŸ’¾ æ­£åœ¨ä¿å­˜å‘é‡æ•°æ®åº“...")

    # === ä½¿ç”¨ Chroma å­˜å‚¨ ===
    db = Chroma(
        collection_name="issues",
        persist_directory=persist_dir,
        embedding_function=embeddings
    )

    # é€æ¡å†™å…¥ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
    for text, meta in tqdm(zip(texts, metadatas), total=len(texts), desc="ğŸ“š å†™å…¥ Chroma", dynamic_ncols=True):
        db.add_texts([text], metadatas=[meta])

    print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ï¼š{persist_dir}")
    return db


# ===== å‘½ä»¤è¡Œå…¥å£ =====
def main():
    parser = argparse.ArgumentParser(description="æ„å»º GitHub Issue å‘é‡çŸ¥è¯†åº“")
    parser.add_argument("--repo", default="OpenCSGs/csghub", help="GitHub ä»“åº“åï¼Œä¾‹å¦‚ OpenCSGs/csghub")
    parser.add_argument("--persist_dir", default="data/chroma", help="å‘é‡æ•°æ®åº“ä¿å­˜ç›®å½•")
    parser.add_argument("--max_pages", type=int, default=3, help="æœ€å¤§æ‹‰å–é¡µæ•°")
    parser.add_argument("--token", default=os.getenv("GITHUB_TOKEN"), help="GitHub Tokenï¼Œå¯ä½¿ç”¨ç¯å¢ƒå˜é‡")

    args = parser.parse_args()

    print(f"ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“ï¼š{args.repo}")
    build_vector_db(
        repo_name=args.repo,
        persist_dir=args.persist_dir,
        max_pages=args.max_pages,
        token=args.token
    )


if __name__ == "__main__":
    import dotenv
    dotenv.load_dotenv(".env")
    main()
